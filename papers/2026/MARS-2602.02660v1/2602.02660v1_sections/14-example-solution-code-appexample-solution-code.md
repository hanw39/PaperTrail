# Example Solution Code {#app:example-solution-code}

In this section, we showcase the final solution generated for the iMet-2020-fgvc7 competition. This repository was produced by MARS utilizing Gemini-3-Pro-Preview and achieved the highest validation performance during the search process.

::: promptbox
Module: library/config.py

    import os
    import torch
    import random
    import numpy as np


    class Config:
        # =========================================================================
        # Directories & Paths
        # =========================================================================
        INPUT_DIR = "./input"
        METADATA_DIR = "./metadata"

        # Output directories
        WORKING_DIR = "./working/idea_4"
        SUBMISSION_DIR = "./submission"

        # Metadata Files (Generated by previous steps)
        TRAIN_CSV = os.path.join(METADATA_DIR, "train.csv")
        VAL_CSV = os.path.join(METADATA_DIR, "val.csv")
        TEST_CSV = os.path.join(METADATA_DIR, "test.csv")

        # Raw Data Files
        LABELS_CSV = os.path.join(INPUT_DIR, "labels.csv")
        SAMPLE_SUBMISSION = os.path.join(INPUT_DIR, "sample_submission.csv")

        # =========================================================================
        # Data Configuration
        # =========================================================================
        IMG_SIZE = (352, 352)  # Resolution optimized for detail vs throughput
        NUM_CLASSES = 3474  # Total number of attributes

        # =========================================================================
        # Model Configuration
        # =========================================================================
        # Ensemble members
        MODEL_A_NAME = "resnet101d"  # Texture specialist (Deep Stem)
        MODEL_B_NAME = "convnext_base"  # Context specialist (Large Kernel)

        # =========================================================================
        # Training Hyperparameters
        # =========================================================================
        SEED = 42
        EPOCHS = 12
        BATCH_SIZE = 48  # Tuned for A100 40GB with 352x352 resolution

        # Optimizer settings
        LEARNING_RATE = 1e-3  # Max LR for OneCycle
        WEIGHT_DECAY = 1e-2  # Standard AdamW decay

        # Loss settings
        ASL_GAMMA_NEG = 4.0  # Asymmetric Loss parameters
        ASL_GAMMA_POS = 0.0
        ASL_CLIP = 0.05

        # =========================================================================
        # Compute Environment
        # =========================================================================
        NUM_WORKERS = 8  # 12 vCPUs available, leave some overhead
        DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

        # =========================================================================
        # Debugging & Development
        # =========================================================================
        DEBUG = False  # Set to True to run on a small subset
        DEBUG_SAMPLE_SIZE = 5000

        @classmethod
        def setup(cls):
            """Creates necessary output directories."""
            os.makedirs(cls.WORKING_DIR, exist_ok=True)
            os.makedirs(cls.SUBMISSION_DIR, exist_ok=True)


    def seed_everything(seed=42):
        """Sets the random seed for reproducibility across all libraries."""
        random.seed(seed)
        os.environ["PYTHONHASHSEED"] = str(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False  # False for exact reproducibility


    # Initialize directories on import
    Config.setup()
:::

::: promptbox
Module: library/dataset.py

    import os
    import cv2
    import torch
    import numpy as np
    import pandas as pd
    import albumentations as A
    from albumentations.pytorch import ToTensorV2
    from torch.utils.data import Dataset, DataLoader
    from library.config import Config


    def load_processed_dataframe(mode, load_cached_data=True):
        """
        Loads the metadata dataframe for a specific mode (train/val/test).
        Implements caching using Parquet files to store processed dataframes
        (where attribute_ids strings are converted to lists).
        """
        cache_dir = Config.WORKING_DIR
        os.makedirs(cache_dir, exist_ok=True)
        cache_path = os.path.join(cache_dir, f"cached_{mode}.parquet")

        # 1. Try to load from cache
        if load_cached_data and os.path.exists(cache_path):
            try:
                df = pd.read_parquet(cache_path)
                return df
            except Exception:
                # If load fails, proceed to process from scratch
                pass

        # 2. Process from scratch
        if mode == "train":
            csv_path = Config.TRAIN_CSV
        elif mode == "val":
            csv_path = Config.VAL_CSV
        elif mode == "test":
            csv_path = Config.TEST_CSV
        else:
            raise ValueError(f"Unknown mode: {mode}")

        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"Metadata file not found: {csv_path}")

        df = pd.read_csv(csv_path)

        # Parse attribute_ids: "0 1 2" -> [0, 1, 2]
        # Handle NaNs by converting to empty string first
        df["attribute_ids"] = df["attribute_ids"].fillna("")

        # Function to safe convert string to list of ints
        def parse_ids(x):
            if not x.strip():
                return np.array([], dtype=int)
            return np.array([int(i) for i in x.split()], dtype=int)

        # We store as numpy arrays inside the dataframe cells for parquet compatibility/efficiency
        df["parsed_attributes"] = df["attribute_ids"].apply(parse_ids)

        # 3. Save to cache
        try:
            df.to_parquet(cache_path, index=False)
        except Exception as e:
            print(f"Warning: Failed to save cache to {cache_path}: {e}")

        return df


    class ArtworkDataset(Dataset):
        def __init__(self, df, transforms=None, mode="train"):
            """
            Args:
                df (pd.DataFrame): DataFrame containing metadata.
                transforms (albumentations.Compose): Transforms to apply.
                mode (str): 'train', 'val', or 'test'.
            """
            self.df = df
            self.transforms = transforms
            self.mode = mode
            self.input_dir = Config.INPUT_DIR
            self.num_classes = Config.NUM_CLASSES

        def __len__(self):
            return len(self.df)

        def __getitem__(self, idx):
            row = self.df.iloc[idx]
            image_id = row["id"]
            file_path = row["file_path"]

            # Load Image
            full_path = os.path.join(self.input_dir, file_path)
            image = cv2.imread(full_path)

            if image is None:
                # Fallback for missing/corrupt images: return black image
                # This prevents crashing during training
                image = np.zeros(
                    (Config.IMG_SIZE[0], Config.IMG_SIZE[1], 3), dtype=np.uint8
                )
            else:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # Apply Transforms
            if self.transforms:
                augmented = self.transforms(image=image)
                image = augmented["image"]

            # Create Target (Multi-hot encoding)
            target = torch.zeros(self.num_classes, dtype=torch.float32)

            # In test mode, we might not have valid labels, but we still return a dummy target
            # parsed_attributes is a numpy array of integers
            attr_ids = row["parsed_attributes"]

            if len(attr_ids) > 0:
                # Ensure indices are within bounds
                valid_ids = attr_ids[attr_ids < self.num_classes]
                target[valid_ids] = 1.0

            return image, target, image_id


    def get_transforms(mode="train"):
        """
        Returns the Albumentations transform pipeline for the specified mode.
        """
        img_size = Config.IMG_SIZE

        if mode == "train":
            return A.Compose(
                [
                    A.Resize(height=img_size[0], width=img_size[1]),
                    A.HorizontalFlip(p=0.5),
                    A.Normalize(
                        mean=(0.485, 0.456, 0.406),
                        std=(0.229, 0.224, 0.225),
                        max_pixel_value=255.0,
                        p=1.0,
                    ),
                    ToTensorV2(),
                ]
            )
        else:
            # Validation and Test
            return A.Compose(
                [
                    A.Resize(height=img_size[0], width=img_size[1]),
                    A.Normalize(
                        mean=(0.485, 0.456, 0.406),
                        std=(0.229, 0.224, 0.225),
                        max_pixel_value=255.0,
                        p=1.0,
                    ),
                    ToTensorV2(),
                ]
            )


    def get_dataloaders(
        debug=False,
        batch_size=Config.BATCH_SIZE,
        num_workers=Config.NUM_WORKERS,
        load_cached_data=True,
    ):
        """
        Creates and returns DataLoaders for train, validation, and test sets.

        Args:
            debug (bool): If True, subsets the data for quick debugging.
            batch_size (int): Batch size for the dataloaders.
            num_workers (int): Number of worker processes.
            load_cached_data (bool): Whether to use cached dataframes.

        Returns:
            tuple: (train_loader, val_loader, test_loader)
        """
        # Load DataFrames
        train_df = load_processed_dataframe("train", load_cached_data)
        val_df = load_processed_dataframe("val", load_cached_data)
        test_df = load_processed_dataframe("test", load_cached_data)

        # Debug Subsampling
        if debug:
            debug_size = Config.DEBUG_SAMPLE_SIZE
            train_df = train_df.iloc[:debug_size]
            val_df = val_df.iloc[:debug_size]
            test_df = test_df.iloc[:debug_size]

        # Create Datasets
        train_dataset = ArtworkDataset(
            train_df, transforms=get_transforms("train"), mode="train"
        )
        val_dataset = ArtworkDataset(val_df, transforms=get_transforms("val"), mode="val")
        test_dataset = ArtworkDataset(
            test_df, transforms=get_transforms("test"), mode="test"
        )

        # Create DataLoaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=True,
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=False,
        )

        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
            drop_last=False,
        )

        return train_loader, val_loader, test_loader
:::

::: promptbox
Module: library/inference.py

    import os
    import torch
    import numpy as np
    import pandas as pd
    from tqdm import tqdm

    from library.config import Config
    from library.dataset import get_dataloaders
    from library.models import get_model
    from library.utils import optimize_threshold, load_checkpoint


    def predict_with_tta(model, dataloader, device, mode="val"):
        """
        Generates predictions using Test Time Augmentation (Horizontal Flip).

        Args:
            model (nn.Module): The trained model.
            dataloader (DataLoader): DataLoader for validation or test set.
            device (torch.device): Device to run inference on.
            mode (str): 'val' or 'test'.

        Returns:
            tuple: (all_probs, all_targets_or_ids)
                - all_probs: np.ndarray of shape (N, num_classes)
                - all_targets_or_ids: np.ndarray of targets (if val) or list of ids (if test)
        """
        model.eval()
        all_probs = []
        all_targets_or_ids = []

        with torch.no_grad():
            for batch in dataloader:
                images, targets, ids = batch
                images = images.to(device)

                # 1. Forward pass original
                out_orig = model(images)
                probs_orig = torch.sigmoid(out_orig)

                # 2. Forward pass flipped (TTA)
                # Flip along width dimension (dim 3 for NCHW)
                images_flipped = torch.flip(images, dims=[3])
                out_flipped = model(images_flipped)
                probs_flipped = torch.sigmoid(out_flipped)

                # 3. Average probabilities
                avg_probs = (probs_orig + probs_flipped) / 2.0

                all_probs.append(avg_probs.cpu().numpy())

                # Collect targets or IDs based on mode
                if mode == "test":
                    all_targets_or_ids.extend(ids)
                else:
                    all_targets_or_ids.append(targets.numpy())

        all_probs = np.concatenate(all_probs, axis=0)

        if mode != "test":
            all_targets_or_ids = np.concatenate(all_targets_or_ids, axis=0)

        return all_probs, all_targets_or_ids


    def get_model_predictions(model_name, mode, dataloader, device, load_cached_data=True):
        """
        Gets predictions for a specific model and mode (val/test).
        Implements caching of the raw probability arrays.

        Args:
            model_name (str): Name of the model architecture.
            mode (str): 'val' or 'test'.
            dataloader (DataLoader): The data loader.
            device (torch.device): Compute device.
            load_cached_data (bool): Whether to use cached .npy files.

        Returns:
            tuple: (probs, targets_or_ids)
        """
        cache_dir = Config.WORKING_DIR
        os.makedirs(cache_dir, exist_ok=True)

        probs_path = os.path.join(cache_dir, f"{model_name}_{mode}_probs.npy")
        meta_path = os.path.join(
            cache_dir, f"{model_name}_{mode}_meta.npy"
        )  # targets or ids

        # Try to load from cache
        if load_cached_data and os.path.exists(probs_path) and os.path.exists(meta_path):
            print(f"Loading cached predictions for {model_name} ({mode})...")
            try:
                probs = np.load(probs_path)
                meta = np.load(meta_path, allow_pickle=True)

                if len(probs) != len(dataloader.dataset):
                    raise ValueError(
                        f"Cache size mismatch: {len(probs)} vs {len(dataloader.dataset)}"
                    )

                return probs, meta
            except Exception as e:
                print(f"Failed to load cache: {e}. Re-running inference.")

        # Run inference
        print(f"Running inference for {model_name} ({mode})...")

        # Load model and checkpoint
        model = get_model(model_name, num_classes=Config.NUM_CLASSES, pretrained=False)
        model = model.to(device)

        checkpoint_filename = f"{model_name}_best.pth"
        try:
            load_checkpoint(model, checkpoint_filename, device=device)
        except FileNotFoundError:
            print(
                f"Warning: Checkpoint {checkpoint_filename} not found. Using random weights (for debugging only)."
            )

        probs, meta = predict_with_tta(model, dataloader, device, mode=mode)

        # Save to cache
        np.save(probs_path, probs)
        np.save(meta_path, meta)

        # Clean up
        del model
        torch.cuda.empty_cache()

        return probs, meta


    def ensemble_predictions(probs_list):
        """
        Averages a list of probability arrays.
        """
        if not probs_list:
            return None
        return np.mean(probs_list, axis=0)


    def generate_submission(debug=Config.DEBUG, load_cached_data=True):
        """
        Main function to generate the submission file.

        1. Loads Val and Test loaders.
        2. Gets predictions for Model A and Model B (with TTA).
        3. Ensembles predictions.
        4. Optimizes threshold on Validation set.
        5. Applies threshold to Test set.
        6. Saves submission.csv.
        """
        device = torch.device(Config.DEVICE)

        # 1. Get DataLoaders
        # We don't need the train loader here
        _, val_loader, test_loader = get_dataloaders(
            debug=debug,
            batch_size=Config.BATCH_SIZE,
            num_workers=Config.NUM_WORKERS,
            load_cached_data=load_cached_data,
        )

        models = [Config.MODEL_A_NAME, Config.MODEL_B_NAME]

        # 2. Validation Inference (for Threshold Calibration)
        print("--- Processing Validation Set ---")
        val_probs_list = []
        val_targets = None

        for model_name in models:
            probs, targets = get_model_predictions(
                model_name, "val", val_loader, device, load_cached_data
            )
            val_probs_list.append(probs)
            val_targets = targets  # Targets should be same for all models

        # Ensemble Validation
        val_ensemble_probs = ensemble_predictions(val_probs_list)

        # Optimize Threshold
        print("Optimizing threshold on ensemble...")
        best_threshold, best_score = optimize_threshold(val_targets, val_ensemble_probs)
        print(f"Optimal Threshold: {best_threshold}")
        print(f"Validation Micro-F1 Score with Optimal Threshold: {best_score}")

        # 3. Test Inference
        print("\n--- Processing Test Set ---")
        test_probs_list = []
        test_ids = None

        for model_name in models:
            probs, ids = get_model_predictions(
                model_name, "test", test_loader, device, load_cached_data
            )
            test_probs_list.append(probs)
            test_ids = ids  # IDs should be same for all models

        # Ensemble Test
        test_ensemble_probs = ensemble_predictions(test_probs_list)

        # 4. Generate Submission CSV
        print(f"Generating submission with threshold {best_threshold}...")

        # Binarize predictions
        predictions_bin = (test_ensemble_probs > best_threshold).astype(int)

        submission_rows = []
        for idx, image_id in enumerate(test_ids):
            # Get indices of positive classes
            pred_indices = np.where(predictions_bin[idx] == 1)[0]

            # Format as space-separated string
            pred_str = " ".join(map(str, pred_indices))

            submission_rows.append({"id": image_id, "attribute_ids": pred_str})

        submission_df = pd.DataFrame(submission_rows)

        # Save
        out_path = os.path.join(Config.SUBMISSION_DIR, "submission.csv")
        submission_df.to_csv(out_path, index=False)
        print(f"Submission saved to {out_path}")

        return best_score
:::

::: promptbox
Module: library/loss.py

    import torch
    import torch.nn as nn
    from library.config import Config


    class AsymmetricLoss(nn.Module):
        """
        Asymmetric Loss (ASL) for multi-label classification.

        ASL optimizes the trade-off between precision and recall by decoupling the
        loss components for positive and negative samples. It down-weights easy
        negatives (which are dominant in multi-label settings) to focus learning
        on hard negatives and positive samples.

        Reference: "Asymmetric Loss For Multi-Label Classification" (ICCV 2021)
        """

        def __init__(
            self,
            gamma_neg=Config.ASL_GAMMA_NEG,
            gamma_pos=Config.ASL_GAMMA_POS,
            clip=Config.ASL_CLIP,
            eps=1e-8,
            reduction="mean",
        ):
            """
            Args:
                gamma_neg (float): Focusing parameter for negative samples (down-weights easy negatives).
                gamma_pos (float): Focusing parameter for positive samples.
                clip (float): Probability margin for shifting negative samples (hard thresholding).
                eps (float): Small constant for numerical stability in logarithms.
                reduction (str): Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.
            """
            super(AsymmetricLoss, self).__init__()
            self.gamma_neg = gamma_neg
            self.gamma_pos = gamma_pos
            self.clip = clip
            self.eps = eps
            self.reduction = reduction

        def forward(self, x, y):
            """
            Args:
                x (torch.Tensor): Logits (before sigmoid) of shape (N, C).
                y (torch.Tensor): Ground truth labels of shape (N, C) (0 or 1).

            Returns:
                torch.Tensor: Calculated loss.
            """
            # Explicit casting to float32 is crucial to prevent NaN during mixed-precision training
            # Logits and targets must be in float32 for stable log/pow computations
            x = x.float()
            y = y.float()

            # Calculate probabilities
            xs_pos = torch.sigmoid(x)

            # --- Positive Component ---
            # Standard Focal Loss term for positives: -y * (1-p)^gamma_pos * log(p)
            # We clamp the input to log to avoid log(0)
            loss_pos = (
                y
                * torch.pow(1.0 - xs_pos, self.gamma_pos)
                * torch.log(xs_pos.clamp(min=self.eps))
            )

            # --- Negative Component ---
            # ASL modification: Shifted probability for negatives
            # p_m = max(p - clip, 0)
            # This hard-thresholds easy negatives (where p < clip) to have 0 loss and 0 gradient
            xs_neg = xs_pos
            if self.clip > 0:
                xs_neg = (xs_neg - self.clip).clamp(min=0)

            # Negative term: -(1-y) * (p_m)^gamma_neg * log(1 - p_m)
            loss_neg = (
                (1.0 - y)
                * torch.pow(xs_neg, self.gamma_neg)
                * torch.log((1.0 - xs_neg).clamp(min=self.eps))
            )

            # Combine components
            # Note: The negative signs from the formulas are applied here
            loss = -(loss_pos + loss_neg)

            # Apply reduction
            if self.reduction == "mean":
                return loss.mean()
            elif self.reduction == "sum":
                return loss.sum()
            else:
                return loss
:::

::: promptbox
Module: library/models.py

    import timm
    import torch.nn as nn
    from library.config import Config


    def get_model(model_name, num_classes=Config.NUM_CLASSES, pretrained=True):
        """
        Creates and returns a model architecture based on the provided name using the timm library.

        This function instantiates the backbone (e.g., ResNet101d, ConvNeXt-Base) and
        replaces the classification head to match the specific number of classes in the dataset.
        It adheres to the strategy of using Global Average Pooling followed by a Linear projection,
        which is the default behavior of timm's create_model when num_classes is specified.

        Args:
            model_name (str): Name of the model architecture (e.g., 'resnet101d', 'convnext_base').
            num_classes (int): Number of output classes (attributes). Defaults to Config.NUM_CLASSES.
            pretrained (bool): Whether to load pretrained ImageNet weights. Defaults to True.

        Returns:
            nn.Module: The instantiated PyTorch model.
        """
        try:
            # Instantiate the model using timm
            # num_classes argument automatically resets the classifier head to the correct size
            # and initializes it randomly.
            model = timm.create_model(
                model_name, pretrained=pretrained, num_classes=num_classes
            )

            return model

        except Exception as e:
            raise RuntimeError(f"Failed to create model '{model_name}' using timm: {e}")
:::

::: promptbox
Module: library/train.py

    import os
    import time
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import numpy as np
    from torch.cuda.amp import GradScaler, autocast
    from torch.optim.lr_scheduler import OneCycleLR

    from library.config import Config, seed_everything
    from library.dataset import get_dataloaders
    from library.models import get_model
    from library.loss import AsymmetricLoss
    from library.utils import calculate_f1_score, optimize_threshold, save_checkpoint


    class Trainer:
        """
        Manages the training and validation process for a single model.
        """

        def __init__(self, model, optimizer, scheduler, criterion, device, scaler):
            self.model = model
            self.optimizer = optimizer
            self.scheduler = scheduler
            self.criterion = criterion
            self.device = device
            self.scaler = scaler
            self.best_score = 0.0

        def train_epoch(self, train_loader, epoch):
            """
            Runs one epoch of training.
            """
            self.model.train()
            running_loss = 0.0
            num_batches = len(train_loader)

            for i, (images, targets, _) in enumerate(train_loader):
                images = images.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)

                self.optimizer.zero_grad()

                # Mixed precision training
                with autocast():
                    outputs = self.model(images)
                    loss = self.criterion(outputs, targets)

                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()

                if self.scheduler is not None:
                    self.scheduler.step()

                running_loss += loss.item()

            avg_loss = running_loss / num_batches
            return avg_loss

        def validate(self, val_loader):
            """
            Runs validation and calculates metrics.
            """
            self.model.eval()
            running_loss = 0.0
            all_preds = []
            all_targets = []

            with torch.no_grad():
                for images, targets, _ in val_loader:
                    images = images.to(self.device, non_blocking=True)
                    targets = targets.to(self.device, non_blocking=True)

                    # Mixed precision inference
                    with autocast():
                        outputs = self.model(images)
                        loss = self.criterion(outputs, targets)

                    running_loss += loss.item()

                    # Apply sigmoid for probabilities
                    probs = torch.sigmoid(outputs)

                    all_preds.append(probs.cpu().numpy())
                    all_targets.append(targets.cpu().numpy())

            avg_loss = running_loss / len(val_loader)
            all_preds = np.concatenate(all_preds, axis=0)
            all_targets = np.concatenate(all_targets, axis=0)

            # Calculate F1 Score (Standard 0.5 threshold)
            f1_score = calculate_f1_score(all_targets, all_preds, threshold=0.5)

            # Calculate Optimized F1 Score (for monitoring potential)
            best_thresh, opt_f1 = optimize_threshold(all_targets, all_preds, num_steps=50)

            return avg_loss, f1_score, opt_f1, best_thresh

        def fit(self, train_loader, val_loader, epochs, model_name, patience=5):
            """
            Main training loop with early stopping.
            """
            print(f"Starting training for {model_name}...")
            patience_counter = 0

            for epoch in range(1, epochs + 1):
                start_time = time.time()

                # Train
                train_loss = self.train_epoch(train_loader, epoch)

                # Validate
                val_loss, val_f1, opt_f1, best_thresh = self.validate(val_loader)

                elapsed = time.time() - start_time

                # Print full precision metrics
                print(f"Epoch {epoch}/{epochs} | Time: {elapsed:.2f}s")
                print(f"  Train Loss: {train_loss}")
                print(f"  Val Loss:   {val_loss}")
                print(f"  Val F1:     {val_f1}")
                print(f"  Opt F1:     {opt_f1} (Thresh: {best_thresh})")

                # Checkpointing
                # We use the standard F1 (0.5) or Optimized F1?
                # Usually for multi-label with unknown distribution shifts, optimizing threshold is safer.
                # However, for consistency with the prompt's metric definition, we track improvement on Opt F1
                # as we will calibrate threshold in inference.
                current_score = opt_f1

                if current_score > self.best_score:
                    print(
                        f"  Score improved from {self.best_score} to {current_score}. Saving checkpoint."
                    )
                    self.best_score = current_score
                    save_checkpoint(
                        self.model,
                        self.optimizer,
                        epoch,
                        self.best_score,
                        f"{model_name}_best.pth",
                    )
                    patience_counter = 0
                else:
                    patience_counter += 1
                    print(f"  No improvement. Patience: {patience_counter}/{patience}")

                if patience_counter >= patience:
                    print(f"Early stopping triggered at epoch {epoch}.")
                    break

            return self.best_score


    def train_specific_model(model_name, epochs=Config.EPOCHS, debug=Config.DEBUG):
        """
        Sets up the environment and trains a specific model architecture.

        Args:
            model_name (str): Name of the model to train (e.g., 'resnet101d').
            epochs (int): Number of epochs to train.
            debug (bool): Whether to run in debug mode (subset of data).
        """
        # 1. Setup
        seed_everything(Config.SEED)
        device = torch.device(Config.DEVICE)

        print(f"Initializing {model_name} on {device}...")

        # 2. Data
        # load_cached_data=True ensures we use the parquet cache if available
        train_loader, val_loader, _ = get_dataloaders(
            debug=debug,
            batch_size=Config.BATCH_SIZE,
            num_workers=Config.NUM_WORKERS,
            load_cached_data=True,
        )

        # 3. Model
        model = get_model(model_name, num_classes=Config.NUM_CLASSES, pretrained=True)
        model = model.to(device)

        # 4. Optimizer & Scheduler
        optimizer = optim.AdamW(
            model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY
        )

        # OneCycleLR needs total steps
        steps_per_epoch = len(train_loader)
        scheduler = OneCycleLR(
            optimizer,
            max_lr=Config.LEARNING_RATE,
            epochs=epochs,
            steps_per_epoch=steps_per_epoch,
            pct_start=0.1,  # Warmup for first 10%
            div_factor=25.0,
            final_div_factor=1000.0,
        )

        # 5. Loss & Scaler
        criterion = AsymmetricLoss(
            gamma_neg=Config.ASL_GAMMA_NEG,
            gamma_pos=Config.ASL_GAMMA_POS,
            clip=Config.ASL_CLIP,
        )
        scaler = GradScaler()

        # 6. Trainer
        trainer = Trainer(model, optimizer, scheduler, criterion, device, scaler)

        # 7. Execute
        best_score = trainer.fit(train_loader, val_loader, epochs, model_name)

        print(f"Training finished for {model_name}. Best F1 Score: {best_score}")

        # Clear memory
        del model, optimizer, scheduler, scaler, trainer, train_loader, val_loader
        torch.cuda.empty_cache()

        return best_score
:::

::: promptbox
Module: library/utils.py

    import os
    import torch
    import numpy as np
    from sklearn.metrics import f1_score
    from library.config import Config


    def calculate_f1_score(y_true, y_pred, threshold=0.5):
        """
        Calculates the Micro-averaged F1 score.

        Args:
            y_true (np.ndarray or torch.Tensor): Ground truth labels (N, C).
            y_pred (np.ndarray or torch.Tensor): Predicted probabilities (N, C).
            threshold (float): Threshold for binarizing predictions.

        Returns:
            float: Micro F1 score.
        """
        # Ensure inputs are numpy arrays
        if isinstance(y_true, torch.Tensor):
            y_true = y_true.detach().cpu().numpy()
        if isinstance(y_pred, torch.Tensor):
            y_pred = y_pred.detach().cpu().numpy()

        # Binarize predictions based on the threshold
        y_pred_bin = (y_pred > threshold).astype(int)

        # Calculate Micro-F1 score
        return f1_score(y_true, y_pred_bin, average="micro")


    def optimize_threshold(y_true, y_pred, num_steps=100):
        """
        Finds the optimal decision threshold for Micro F1 score via linear search.

        Args:
            y_true (np.ndarray or torch.Tensor): Ground truth labels.
            y_pred (np.ndarray or torch.Tensor): Predicted probabilities.
            num_steps (int): Number of steps in the linear search (between 0 and 1).

        Returns:
            tuple: (best_threshold, best_score)
        """
        if isinstance(y_true, torch.Tensor):
            y_true = y_true.detach().cpu().numpy()
        if isinstance(y_pred, torch.Tensor):
            y_pred = y_pred.detach().cpu().numpy()

        best_threshold = 0.5
        best_score = -1.0

        # Search range from 0.01 to 0.99 to avoid edge cases
        thresholds = np.linspace(0.01, 0.99, num_steps)

        for thresh in thresholds:
            score = calculate_f1_score(y_true, y_pred, threshold=thresh)
            if score > best_score:
                best_score = score
                best_threshold = thresh

        return best_threshold, best_score


    def save_checkpoint(model, optimizer, epoch, score, filename):
        """
        Saves a model checkpoint to the working directory.

        Args:
            model (torch.nn.Module): The model to save.
            optimizer (torch.optim.Optimizer): The optimizer state.
            epoch (int): Current epoch.
            score (float): Validation score (F1).
            filename (str): Name of the file to save.
        """
        # Ensure the working directory exists
        os.makedirs(Config.WORKING_DIR, exist_ok=True)

        filepath = os.path.join(Config.WORKING_DIR, filename)

        state = {
            "epoch": epoch,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": (
                optimizer.state_dict() if optimizer is not None else None
            ),
            "score": score,
        }

        torch.save(state, filepath)


    def load_checkpoint(model, filename, optimizer=None, device=Config.DEVICE):
        """
        Loads a model checkpoint.

        Args:
            model (torch.nn.Module): The model to load weights into.
            filename (str): Name of the checkpoint file (relative to WORKING_DIR or absolute).
            optimizer (torch.optim.Optimizer, optional): Optimizer to load state into.
            device (str): Device to map location to.

        Returns:
            tuple: (epoch, score)
        """
        # Determine full path
        if os.path.exists(filename):
            filepath = filename
        else:
            filepath = os.path.join(Config.WORKING_DIR, filename)

        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Checkpoint file not found: {filepath}")

        # Load checkpoint
        checkpoint = torch.load(filepath, map_location=device)

        # Load model state
        model.load_state_dict(checkpoint["model_state_dict"])

        # Load optimizer state if provided and available
        if optimizer is not None and checkpoint.get("optimizer_state_dict") is not None:
            optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

        epoch = checkpoint.get("epoch", 0)
        score = checkpoint.get("score", 0.0)

        return epoch, score
:::

::: promptbox
Main Script: main.py

    import sys
    import os
    import numpy as np
    import pandas as pd
    import torch
    import cv2
    from library.config import Config, seed_everything
    from library.train import train_specific_model
    from library.inference import (
        get_model_predictions,
        ensemble_predictions,
        generate_submission,
    )
    from library.utils import optimize_threshold
    from library.dataset import get_dataloaders


    def main():
        # 1. Setup
        seed_everything(Config.SEED)
        device = torch.device(Config.DEVICE)

        print("=== Starting Orchestration Script ===")

        # 2. Train Models
        # We train for 5 epochs to improve convergence while staying within the 4-hour limit.
        # Cite Lesson 00003: Longer schedule allows better convergence.
        # Cite Lesson 00018: Ensemble of diverse models.
        print(f"\n[Training] Starting training for Model A: {Config.MODEL_A_NAME}")
        train_specific_model(Config.MODEL_A_NAME, epochs=5, debug=False)

        print(f"\n[Training] Starting training for Model B: {Config.MODEL_B_NAME}")
        train_specific_model(Config.MODEL_B_NAME, epochs=5, debug=False)

        # 3. Validation & Metric Calculation
        print("\n[Validation] Loading validation data...")
        # We only need the validation loader here to get targets and run inference
        _, val_loader, _ = get_dataloaders(debug=False, load_cached_data=True)

        print("[Validation] Generating ensemble predictions...")
        # Get predictions for Model A
        # load_cached_data=False ensures we use the newly trained weights
        probs_a, targets = get_model_predictions(
            Config.MODEL_A_NAME, "val", val_loader, device, load_cached_data=False
        )
        # Get predictions for Model B
        probs_b, _ = get_model_predictions(
            Config.MODEL_B_NAME, "val", val_loader, device, load_cached_data=False
        )

        # Ensemble predictions (simple average)
        ensemble_probs = ensemble_predictions([probs_a, probs_b])

        # Ensure targets are numpy array
        if isinstance(targets, torch.Tensor):
            targets = targets.cpu().numpy()
        elif isinstance(targets, list):
            targets = np.array(targets)

        # Optimize threshold
        best_thresh, best_score = optimize_threshold(targets, ensemble_probs)

        # REQUIRED OUTPUT
        print(f"Final Validation Metric: {best_score}")

        # 4. Failure Analysis
        print("\n[Analysis] Performing Failure Analysis...")

        # Calculate per-sample F1 to determine error magnitude
        # Binarize predictions using the optimal threshold
        preds_bin = (ensemble_probs > best_thresh).astype(int)

        # Calculate F1 per sample (instance-level)
        # F1 = 2*TP / (2*TP + FP + FN)
        tp = np.sum((preds_bin == 1) & (targets == 1), axis=1)
        fp = np.sum((preds_bin == 1) & (targets == 0), axis=1)
        fn = np.sum((preds_bin == 0) & (targets == 1), axis=1)

        epsilon = 1e-7
        f1_samples = (2 * tp) / (2 * tp + fp + fn + epsilon)
        error_magnitude = 1.0 - f1_samples

        # Load metadata to get features
        val_df = pd.read_csv(Config.VAL_CSV)

        # Feature 1: Label Cardinality (from ground truth)
        # Handle potential NaNs in attribute_ids
        val_df["attribute_ids"] = val_df["attribute_ids"].fillna("")
        val_df["num_labels"] = val_df["attribute_ids"].apply(
            lambda x: len(x.split()) if x.strip() else 0
        )

        # Feature 2: Image Brightness (from image content)
        # We process a subset of validation images to save time
        sample_size = min(2000, len(val_df))
        sample_indices = np.random.choice(len(val_df), size=sample_size, replace=False)

        brightness_values = []
        sampled_errors = []
        sampled_cardinality = []

        print(f"  Processing {sample_size} images for feature extraction...")
        for idx in sample_indices:
            row = val_df.iloc[idx]
            path = os.path.join(Config.INPUT_DIR, row["file_path"])

            # Read image
            img = cv2.imread(path)
            if img is not None:
                # Calculate mean brightness
                b = np.mean(img) / 255.0
                brightness_values.append(b)
                sampled_errors.append(error_magnitude[idx])
                sampled_cardinality.append(val_df.iloc[idx]["num_labels"])

        # Calculate Correlations
        if len(sampled_errors) > 1:
            # Correlation with Label Cardinality
            corr_card = np.corrcoef(sampled_cardinality, sampled_errors)[0, 1]
            print(f"Correlation (Error vs Label Cardinality): {corr_card:.4f}")

            # Correlation with Brightness
            corr_bright = np.corrcoef(brightness_values, sampled_errors)[0, 1]
            print(f"Correlation (Error vs Image Brightness): {corr_bright:.4f}")
        else:
            print("Insufficient data for correlation analysis.")

        # 5. Submission
        THRESHOLD_SCORE = 0.6335639211171432

        print(f"\n[Submission] Checking threshold: {best_score} > {THRESHOLD_SCORE}")

        if best_score > THRESHOLD_SCORE:
            print("Threshold met. Generating submission file...")
            # generate_submission handles test inference and saving
            # load_cached_data=False ensures we generate fresh test predictions
            generate_submission(debug=False, load_cached_data=False)
        else:
            print("Threshold not met. Skipping submission generation.")

        print("\n=== Workflow Completed ===")


    if __name__ == "__main__":
        main()
:::

[^1]: <https://github.com/openai/mle-bench/tree/main>

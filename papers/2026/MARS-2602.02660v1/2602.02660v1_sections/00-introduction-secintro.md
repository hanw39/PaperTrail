# Introduction {#sec:intro}

The integration of Large Language Models (LLMs) into software engineering has fundamentally transformed code generation, evolving from simple auto-completion to autonomous agents capable of resolving GitHub issues [@jimenez2023swe; @yang2024swe] and generating functional scripts [@li2022competition; @wang2024openhands; @aide2025]. However, while current agents excel at general software maintenance tasks -- such as patching bugs or writing unit tests -- they face significant hurdles when applied to the domain of Automating AI Research [@chan2024mle; @tian2024scicode; @wijk2024re; @yamada2025ai; @starace2025paperbench]. Unlike standard software development, where correctness is often binary and verification is computationally cheap, AI research is a probabilistic, resource-intensive endeavor. It requires not only coding intelligence but also the strategic foresight to navigate a landscape defined by computationally expensive evaluations, opaque performance attribution, and high architectural complexity.

Existing agentic frameworks, designed primarily for monolithic code generation, struggle to adapt to these constraints. First, they typically view problem-solving as a purely code-based challenge [@huang2023mlagentbench; @aide2025; @toledo2025ai], ignoring the economic reality of research: model training and data processing consume vast computational resources. An agent that improves model accuracy by 0.1% but increases training time from one hour to ten hours is often practically useless, yet standard search algorithms would prioritize it. Second, the monolithic and unstructured scripts often produced by previous LLM agents are fragile and ill-suited for the modular complexity required in research repositories, where data loading, model architecture, and training loops must interact seamlessly. Finally, research progress is iterative and opaque; when a new experiment yields better results, it is difficult to isolate the causal factor. Standard memory-based agents [@packer2023memgpt; @shinn2023reflexion; @xu2025mem; @ouyang2025reasoningbank] lack the mechanism to solve this *credit assignment problem*, often failing to learn effectively from past trials.

To bridge this gap, we introduce **MARS** (**M**odular **A**gent with **R**eflective **S**earch), a framework explicitly optimized for the distinct constraints of autonomous scientific discovery. MARS reformulates the research process as a search for an optimal software repository, governed by three core pillars. To address the high cost of evaluation, we employ *Budget-Aware Planning* via a cost-constrained Monte Carlo Tree Search (MCTS). Unlike general search algorithms, our method explicitly balances performance maximization with execution expense, prioritizing efficient solutions -- such as favoring a 1-hour training run over a 4-hour run if performance is comparable -- to optimize the discovery rate within a fixed budget. To manage architectural complexity, we replace fragile scripting with a *Modular "Design-Decompose-Implement"* pipeline. This structure employs specialized agents to architect solutions into independent, testable modules. Finally, to resolve the credit assignment problem, we introduce *Comparative Reflective Memory*. By analyzing the differences between the current solution and the best-known solution, the agent distills high-signal, causal insights, isolating the specific factors driving performance shifts in a way that standard memory mechanisms cannot. As illustrated in Figure [1](#fig:aha-moment){reference-type="ref" reference="fig:aha-moment"}, these pillars allow MARS to experience "Aha!" moments during long-horizon exploration, successfully navigating complex optimization landscapes where baselines fail.

<figure id="fig:aha-moment" data-latex-placement="tb">
<embed src="./figures/mars-aha-moment.pdf" />
<figcaption>The “Aha!” moment of MARS on the challenging iMet-2020-FGVC7 task. The visualization tracks validation performance gains triggered by specific strategic lessons. While existing methods fail to reach medal-level performance, MARS progressively refines its strategy – evolving from a lightweight residual network to model ensemble techniques – to ultimately achieve a silver medal.</figcaption>
</figure>

Our contributions are summarized as follows:

- We introduce **MARS**, a framework designed for automated AI research, featuring a novel combination of Budget-Aware MCTS, a modular implementation pipeline, and Comparative Reflective Memory.

- We perform extensive evaluation on the MLE-Bench benchmark, where MARS achieves state-of-the-art performance among open-source frameworks under comparable settings. Ablation studies further validate the necessity of each proposed mechanism.

- We provide qualitative analyses of how MARS drives long-horizon exploration. To facilitate future research, we release prompts in Appendix [13](#app:agent-prompts){reference-type="ref" reference="app:agent-prompts"}, and MARS generated code, trajectories in <https://github.com/jfc43/MARS>.
